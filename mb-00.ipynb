{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nXS4RUQQugV"
   },
   "source": [
    "# Tutorial Part-of-Speech tagging  Con Deep Learning\n",
    "\n",
    "### En este tutorial, veremos cómo puede usar un modelo simple en Keras, para entrenar y evaluar una red neuronal artificial para problemas de clasificación de múltiples clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIheRrq2Quga"
   },
   "source": [
    "## PARTE 1  -  Pre-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw10qukzQuge"
   },
   "outputs": [],
   "source": [
    "# Asegurar reproducibilidad\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "CUSTOM_SEED = 42\n",
    "np.random.seed(CUSTOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxOcxDUmJcvL"
   },
   "source": [
    "### Descargamos el Corpus Ancora - Cess_esp del nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "NILevBJxQugr",
    "outputId": "53a010b3-acf6-419f-c28f-cad9f8f49404"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to /home/orlando/nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eqdUvUCEJjgc"
   },
   "source": [
    "### Extraemos las oraciones tageadas del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ss3RHo4LQugx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "tagged_sentences = cess_esp.tagged_sents()\n",
    "#print('a random sentence: \\n-> {}'.format(random.choice(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCW_ENdLJudC"
   },
   "source": [
    "### Extraemos los datos de la cantidad de oraciones a ser usadas y un ejemplo de una oracion presente en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "2clQUNdtQug4",
    "outputId": "153a9f86-fa43-4df7-c5d3-c103a6d35de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')]\n",
      "Tagged sentences:  6030\n",
      "Tagged words: 192685\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(cess_esp.tagged_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_aFjuCQKG1O"
   },
   "source": [
    "### Se procede a Dividir en una lista de Oraciones dividida en lista de palabras y cada palabra con un correspondiente tag en un alista diferente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "516a_v5vQuhC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "sentences, tagss =[], [] \n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    tagss.append(np.array(tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UN_E4ePpKhvy"
   },
   "source": [
    "### Imprimimos una posicion de la lista como ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "l6uGGSqZQuhM",
    "outputId": "e458cb93-5e6b-482f-e3f3-e08c02f2ec7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EDF' 'tiene' 'previsto' 'invertir' '194' 'millones' 'de' 'euros' '-Fpa-'\n",
      " '186' 'millones' 'de' 'dólares' '-Fpt-' 'en' 'la' 'central' 'de'\n",
      " 'Río_Bravo' ',' 'con' 'una' 'potencia' 'de' '495' 'megavatios' ',' 'y'\n",
      " '134' 'millones' 'de' 'euros' '-Fpa-' '28' 'millones' 'de' 'dólares'\n",
      " '-Fpt-' 'en' 'Saltillo' ',' 'que' 'como' 'la' 'primera' 'funcionará'\n",
      " 'con' 'gas' 'natural' 'y' 'cuya' 'potencia' 'prevista' 'es' 'de' '247'\n",
      " 'megavatios' '.']\n",
      "['np00000' 'vmip3s0' 'aq0msp' 'vmn0000' 'Z' 'ncmp000' 'sps00' 'Zm' 'Fpa'\n",
      " 'Z' 'ncmp000' 'sps00' 'Zm' 'Fpt' 'sps00' 'da0fs0' 'ncfs000' 'sps00'\n",
      " 'np00000' 'Fc' 'sps00' 'di0fs0' 'ncfs000' 'sps00' 'Z' 'ncmp000' 'Fc' 'cc'\n",
      " 'Z' 'ncmp000' 'sps00' 'Zm' 'Fpa' 'Z' 'ncmp000' 'sps00' 'Zm' 'Fpt' 'sps00'\n",
      " 'np00000' 'Fc' 'pr0cn000' 'cs' 'da0fs0' 'ao0fs0' 'vmif3s0' 'sps00'\n",
      " 'ncms000' 'aq0cs0' 'cc' 'pr0fs000' 'ncfs000' 'aq0fsp' 'vsip3s0' 'sps00'\n",
      " 'Z' 'ncmp000' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[5])\n",
    "print(tagss[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WFQrAblFQuhT"
   },
   "source": [
    "### Dividimos el corpus de la siguiente manera, Utilizamos aproximadamente el 60% de las oraciones etiquetadas para el entrenamiento, el 20% como conjunto de validación y el 20% para evaluar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtLulrEOQuhU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "(training_sentences, \n",
    " test_sentences, \n",
    " training_tags, \n",
    " test_tags) = train_test_split(sentences, tagss, test_size=0.2)\n",
    "\n",
    "(train_sentences, \n",
    " eval_sentences, \n",
    " train_tags, \n",
    " eval_tags) = train_test_split(training_sentences, training_tags, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Mk0scnsK1OE"
   },
   "source": [
    "### Imprimimos los tamaños de las listas que nos indicaran el tamaño de filas de las matrices con las que estaremos trabajando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "7HkjbP_IQuhZ",
    "outputId": "6dde887d-df6a-4dff-9a3b-48d2041085ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_sentences:4824\n",
      "train_sentences: 3618\n",
      "test_sentences: 1206\n",
      "eval_sentences: 1206\n",
      "\n",
      "['*' 'El' 'Madrid' 'precisa' 'que' 'el' 'Deportivo' 'gane' 'la' 'Liga' ','\n",
      " 'porque' 'los' 'gallegos' 'no' 'son' 'considerados' 'unos' 'herederos'\n",
      " ',' 'sino' 'unos' 'entrometidos' 'que' 'se' 'supone' 'temporales' ','\n",
      " 'que' 'pertenecen' 'a' 'la' 'actualidad' 'más' 'rabiosa' 'y' 'no' 'a'\n",
      " 'la' 'historia' 'más' 'enrabietada' '.']\n",
      "['El' 'técnico' 'barcelonista' 'ha' 'asegurado' 'que' 'la' 'visita' 'de'\n",
      " 'Gaspart' 'ha' 'contribuido' 'a' '\"' 'sumar' '\"' ',' 'y' '*0*' 'ha'\n",
      " 'argumentado' 'que' 'el' 'encuentro' 'con' 'el' 'presidente' 'significa'\n",
      " 'que' 'en' 'el' 'Barcelona' '\"' 'todos' 'van' 'en' 'la' 'misma'\n",
      " 'dirección' '\"' '.']\n",
      "['Lo_suyo' ',' 'lo' 'de' 'las' 'ratas' ',' 'no' 'es' 'la' 'carroña' 'pura'\n",
      " 'y' 'dura' 'sino' 'la' 'vida' 'regalada' ',' 'el' 'eterno' 'banquete'\n",
      " 'de' 'sobras' 'y' 'residuos' ',' 'el' 'festín' 'organizado' 'a' 'la'\n",
      " 'sobra' 'de' 'la' 'abundancia' 'y' 'el' 'hartazgo' '.']\n",
      "\n",
      "training_tags:4824\n",
      "train_tags: 3618\n",
      "test_tags: 1206\n",
      "eval_tags: 1206\n",
      "\n",
      "['Fz' 'da0ms0' 'np0000l' 'vmip3s0' 'cs' 'da0ms0' 'np0000o' 'vmsp3s0'\n",
      " 'da0fs0' 'np0000a' 'Fc' 'cs' 'da0mp0' 'ncmp000' 'rn' 'vsip3p0' 'vmp00pm'\n",
      " 'di0mp0' 'ncmp000' 'Fc' 'cc' 'di0mp0' 'ncmp000' 'pr0cn000' 'p0000000'\n",
      " 'vmip3s0' 'aq0cp0' 'Fc' 'pr0cn000' 'vmip3p0' 'sps00' 'da0fs0' 'ncfs000'\n",
      " 'rg' 'aq0fs0' 'cc' 'rn' 'sps00' 'da0fs0' 'ncfs000' 'rg' 'aq0fsp' 'Fp']\n",
      "['da0ms0' 'ncms000' 'aq0cs0' 'vaip3s0' 'vmp00sm' 'cs' 'da0fs0' 'ncfs000'\n",
      " 'sps00' 'np00000' 'vaip3s0' 'vmp00sm' 'sps00' 'Fe' 'vmn0000' 'Fe' 'Fc'\n",
      " 'cc' 'sn.e-SUJ' 'vaip3s0' 'vmp00sm' 'cs' 'da0ms0' 'ncms000' 'sps00'\n",
      " 'da0ms0' 'ncms000' 'vmip3s0' 'cs' 'sps00' 'da0ms0' 'np00000' 'Fe'\n",
      " 'pi0mp000' 'vmip3p0' 'sps00' 'da0fs0' 'di0fs0' 'ncfs000' 'Fe' 'Fp']\n",
      "['px3ns000' 'Fc' 'da0ns0' 'sps00' 'da0fp0' 'ncfp000' 'Fc' 'rn' 'vsip3s0'\n",
      " 'da0fs0' 'ncfs000' 'aq0fs0' 'cc' 'aq0fs0' 'cc' 'da0fs0' 'ncfs000'\n",
      " 'aq0fsp' 'Fc' 'da0ms0' 'aq0ms0' 'ncms000' 'sps00' 'ncfp000' 'cc'\n",
      " 'ncmp000' 'Fc' 'da0ms0' 'ncms000' 'aq0msp' 'sps00' 'da0fs0' 'ncfs000'\n",
      " 'sps00' 'da0fs0' 'ncfs000' 'cc' 'da0ms0' 'ncms000' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(\"training_sentences:\" + str(len(training_sentences)))\n",
    "print(\"train_sentences: \" + str(len(train_sentences)))\n",
    "print(\"test_sentences: \" + str(len(test_sentences)))\n",
    "print(\"eval_sentences: \" + str(len(eval_sentences)) + \"\\n\")\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(test_sentences[0])\n",
    "print(eval_sentences[0])\n",
    "\n",
    "print(\"\\ntraining_tags:\" + str(len(training_sentences)))\n",
    "print(\"train_tags: \" + str(len(train_tags)))\n",
    "print(\"test_tags: \" + str(len(test_tags)))\n",
    "print(\"eval_tags: \" + str(len(eval_tags)) + \"\\n\")\n",
    "\n",
    "print(train_tags[0])\n",
    "print(test_tags[0])\n",
    "print(eval_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jd-i6q85Quho"
   },
   "source": [
    "### Ahora creamos una array con todas las palabras y los tags presentes en el corpus, adicionalmente se crea un diccionario que contiene las palabras unicas y los tags unicos de tal forma que no se repitan y que contienen un indice o llave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdCNulCoQuhr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24499\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "words, tagsss = set([]), set([])\n",
    " \n",
    "for s in (train_sentences + eval_sentences + test_sentences):\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in (train_tags + eval_tags + test_tags):\n",
    "    for t in ts:\n",
    "        tagsss.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 2 for i, t in enumerate(list(tagsss))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "tag2index['-OOV-'] = 1  # The special value used to padding\n",
    "\n",
    "print (len(word2index))\n",
    "print (len(tag2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEA9Ek-GOYOn"
   },
   "source": [
    "### Ahora procedemos a transformar cada uno de los conjuntos de oraciones y tags en vectores numericos, modificando la palabra o tag en un Valor numerico que corresponde a una llave en el diccionario de palbras o tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69eec13kQuh2"
   },
   "outputs": [],
   "source": [
    "train_sentences_X, eval_sentences_X, test_sentences_X, train_tags_y, eval_tags_y, test_tags_y = [], [], [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in eval_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    eval_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    train_tags_y.append(s_int)\n",
    "\n",
    "for s in eval_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    eval_tags_y.append(s_int)\n",
    "\n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    test_tags_y.append(s_int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_lXW1mBPNkf"
   },
   "source": [
    "### Se imprime la longitud de las matrices y una muesta de cada una de las matrices creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "Y5A4d_dzQuh6",
    "outputId": "c51290bf-9aec-4d3e-bd2a-957d9d77f367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitudes de las Matrices:\n",
      "3618\n",
      "1206\n",
      "1206\n",
      "3618\n",
      "1206\n",
      "1206\n",
      "\n",
      "Muestra de Datos presentes en las Matrices con las transformaciones:\n",
      "[1268, 15565, 22843, 19624, 5451, 15565, 1694, 24492, 4005, 13926, 23783, 10235, 10898, 21214, 22546, 2611, 22088, 6381, 16439, 23783, 15983, 6381, 18074, 5451, 7736, 3348, 14539, 23783, 5451, 13230, 4122, 4005, 6693, 10261, 10975, 2215, 22546, 4122, 4005, 11959, 10261, 21792, 19536]\n",
      "[2762, 23783, 20642, 13830, 14514, 4166, 23783, 22546, 21482, 4005, 10960, 20352, 2215, 22429, 15983, 4005, 8754, 7947, 23783, 15565, 22553, 15707, 13830, 21830, 2215, 2991, 23783, 15565, 16130, 2096, 4122, 4005, 984, 13830, 4005, 21659, 2215, 15565, 11611, 19536]\n",
      "[15565, 21645, 15692, 21213, 14483, 5451, 4005, 10707, 13830, 114, 21213, 20844, 4122, 15751, 18692, 15751, 23783, 2215, 5065, 21213, 8584, 5451, 15565, 6466, 11476, 15565, 5832, 3005, 5451, 24158, 15565, 12511, 15751, 2895, 15558, 24158, 4005, 20555, 7317, 15751, 19536]\n",
      "[11, 279, 245, 198, 215, 279, 76, 41, 119, 82, 30, 215, 2, 53, 8, 111, 187, 216, 53, 30, 199, 216, 53, 155, 237, 198, 228, 30, 155, 89, 156, 119, 206, 60, 269, 199, 8, 156, 119, 206, 60, 227, 273]\n",
      "[201, 30, 275, 156, 100, 283, 30, 8, 12, 119, 206, 269, 199, 269, 199, 119, 206, 227, 30, 279, 189, 138, 156, 283, 199, 53, 30, 279, 138, 83, 156, 119, 206, 156, 119, 206, 199, 279, 138, 273]\n",
      "[279, 138, 49, 159, 252, 215, 119, 206, 156, 74, 159, 252, 156, 67, 289, 67, 30, 199, 166, 159, 252, 215, 279, 138, 156, 279, 138, 198, 215, 156, 279, 74, 67, 213, 89, 156, 119, 99, 206, 67, 273]\n"
     ]
    }
   ],
   "source": [
    "print(\"Longitudes de las Matrices:\")\n",
    "print(len(train_sentences_X))\n",
    "print(len(eval_sentences_X))\n",
    "print(len(test_sentences_X))\n",
    "print(len(train_tags_y))\n",
    "print(len(eval_tags_y))\n",
    "print(len(test_tags_y))\n",
    "\n",
    "print(\"\\nMuestra de Datos presentes en las Matrices con las transformaciones:\")\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(eval_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(eval_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWLspkzfQ513"
   },
   "source": [
    "### Se calcula cual es la oracion que mayor cantidad de Palabras contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gif6KsESQuh_",
    "outputId": "e2f25a82-99ff-4c92-deec-1b1bc56e44ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)  # 271"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4ffWaDqRA1_"
   },
   "source": [
    "### Se procede a Normalizar las matrices para que todas contengan el numero de columans, con la longitud maxima de palabras encontradas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1054
    },
    "colab_type": "code",
    "id": "mn7iuMIOQuiI",
    "outputId": "c63ae83d-c3ae-4468-97ac-1052d0c43e38"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aead0722f070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_sentences_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meval_sentences_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_sentences_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_sentences_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_sentences_X = pad_sequences(eval_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_tags_y = pad_sequences(eval_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(eval_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(eval_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elkKsVbBNrYO"
   },
   "source": [
    "### Definimos la funcion con la cual categorizaremos los tags y los covertiremos un vector One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGw5_dPX5xc0"
   },
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOmqn-5ZNg23"
   },
   "source": [
    "### Desarrollamos una prueba de la categorisacion de los tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "lepVNGK5bgc1",
    "outputId": "7f69ae9a-00ad-444f-f719-a39d58f834b0"
   },
   "outputs": [],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE 2  -  Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odDOhtO4NZDd"
   },
   "source": [
    "### Definimos el Modelo Base con el cual se procedera a desarrollar la fase de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "x31rRt8PQuiW",
    "outputId": "d4ec4e41-4d25-4f66-a9e8-b94096c585fd"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XghotI4NG9G"
   },
   "source": [
    "### Se dedarrolla el entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1394
    },
    "colab_type": "code",
    "id": "C0gOhZznbg6V",
    "outputId": "508cc732-151a-4cef-dbbe-1a429d9bc261"
   },
   "outputs": [],
   "source": [
    "model_hist = model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)),\n",
    "                       validation_data=(eval_sentences_X, to_categorical(eval_tags_y, len(tag2index))),\n",
    "                       batch_size=128, \n",
    "                       epochs=40,\n",
    "                       validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE 3  -  Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdSkk8mzM1KN"
   },
   "source": [
    "### Evaluamos el modelo y calculamos el valor de precision con respecto a los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cD-YI5Fgb3Kt",
    "outputId": "9f13729e-19a9-4298-8eb1-0bd6928f7fa3"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAhkgtWHQuij"
   },
   "source": [
    "### Definimos la funcion que nos servira para graficar el comportamiento del modelo en cada epoca del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaBUkInNQuik"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_performance(train_loss, train_acc, train_val_loss, train_val_acc):\n",
    "    \"\"\" Plot model loss and accuracy through epochs. \"\"\"\n",
    "    blue= '#34495E'\n",
    "    green = '#2ECC71'\n",
    "    orange = '#E23B13'\n",
    "    # plot model loss\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(10, 8))\n",
    "    ax1.plot(range(1, len(train_loss) + 1), train_loss, blue, linewidth=5, label='training')\n",
    "    ax1.plot(range(1, len(train_val_loss) + 1), train_val_loss, green, linewidth=5, label='validation')\n",
    "    ax1.set_xlabel('# epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.tick_params('y')\n",
    "    ax1.legend(loc='upper right', shadow=False)\n",
    "    ax1.set_title('Model loss through #epochs', color=orange, fontweight='bold')\n",
    "    # plot model accuracy\n",
    "    ax2.plot(range(1, len(train_acc) + 1), train_acc, blue, linewidth=5, label='training')\n",
    "    ax2.plot(range(1, len(train_val_acc) + 1), train_val_acc, green, linewidth=5, label='validation')\n",
    "    ax2.set_xlabel('# epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.tick_params('y')\n",
    "    ax2.legend(loc='lower right', shadow=False)\n",
    "    ax2.set_title('Model accuracy through #epochs', color=orange, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxQh1AtuQuis"
   },
   "source": [
    "### Procedemos a Graficar el comportamiento del Entrenamiento, tanto del conjuento de entrenamiento como el de validacion con respecto a la cantidad de epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "Gs5f3U1nQuit",
    "outputId": "9680cc69-895c-4647-c6ea-42ceb2e393ec"
   },
   "outputs": [],
   "source": [
    "plot_model_performance(\n",
    "    train_loss=model_hist.history.get('loss', []),\n",
    "    train_acc=model_hist.history.get('acc', []),\n",
    "    train_val_loss=model_hist.history.get('val_loss', []),\n",
    "    train_val_acc=model_hist.history.get('val_acc', [])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqiuy8q4GYjF"
   },
   "source": [
    "### Función que Permite convertir Indices en Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJ6GaLot9yZR"
   },
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT6IIQXrQuix"
   },
   "source": [
    "### Hallamos los valores de F1 score, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6HgbDqqsR4a7",
    "outputId": "fa30a704-8c75-4519-aa3f-2ea1afc70528"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test_sentences_X)\n",
    "log_tokens = logits_to_tokens(prediction, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "predicted = []\n",
    "for s in log_tokens:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    predicted.append(s_int)\n",
    "    \n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "GqTuNxppFNu-",
    "outputId": "006ceea6-f599-4b6e-c649-92aca4b9bc5d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_test = test_tags_y\n",
    "\n",
    "print ('\\nClasification report:\\n', classification_report(y_test[0], predicted[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE 4  -  Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvOz-IShFzRR"
   },
   "source": [
    "### Creamos un pequeño Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_WT1PtS_Qui0",
    "outputId": "8a27f475-6624-4b08-b5db-d93e25811c2c"
   },
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    \"Correr es importante para mi .\".split(),\n",
    "    \"Yo fui a correr todos los dias durante un mes .\".split()\n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5E7-zZdGCjY"
   },
   "source": [
    "### Convertimos el texto en Una entrada para el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "BApB6ScZ9jU8",
    "outputId": "1bc922cd-6634-4d82-c558-5339fdc76053"
   },
   "outputs": [],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    "\n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trNZCjTWGLp-"
   },
   "source": [
    "### Se Ejecuta la predicion con la Entrada del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OX6Bd2Rz9oha"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-XS5z-NGiM-"
   },
   "source": [
    "### Conversion de la Salida del Modelo a un lista de Indices de Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgIutMjq92cp"
   },
   "outputs": [],
   "source": [
    "#print(len(predictions))\n",
    "log_tokens = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "print(log_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmWp09kyGrQC"
   },
   "source": [
    "### Presentacion de los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNMCM8_jSdCL"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "heads1 = test_samples[0]\n",
    "heads2 = test_samples[1]\n",
    "\n",
    "t1 = PrettyTable(heads1)\n",
    "body1 = log_tokens[0][:len(test_samples[0])]\n",
    "t1.add_row(body1)\n",
    "print (t1)\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "t2 = PrettyTable(heads2)\n",
    "body2 = log_tokens[1][:len(test_samples[1])]\n",
    "t2.add_row(body2)\n",
    "print (t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gR8kcV0GwFS"
   },
   "source": [
    "### Tratemos de Mejorar la Precision del Modelo, usando una clase que permita ignorar los Valores de Relleno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwNvdZiE956Y"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxT3F3pHIGHk"
   },
   "source": [
    "### Definimos nuevamente nuestro modelo, agregado la clase Creada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KO4DTAAE-BaS"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),  metrics=['accuracy', ignore_class_accuracy(0)]) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8AiPXp8IVOu"
   },
   "source": [
    "### Procedemos a Entrenar Nuevamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CwgZVEIW-ECo"
   },
   "outputs": [],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnExWbEmIa5U"
   },
   "source": [
    "### Calculamos nuevamente la Precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKOa73gRNMm7"
   },
   "outputs": [],
   "source": [
    "scores2 = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores2[1] * 100}\")   # acc: 99.09751977804825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzeCdasrIkuN"
   },
   "source": [
    "### Relaizamos nuevamente el calculo de F1-score, recall, y precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbqNJa0pIvVT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nri9gFnwIxcN"
   },
   "source": [
    "### Relaizamos nuevamente una prueba con el Ejemplo de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xruHro6L-LT2"
   },
   "outputs": [],
   "source": [
    "predictions1 = model.predict(test_samples_X)\n",
    "log_tokens1  = logits_to_tokens(predictions1, {i: t for t, i in tag2index.items()})\n",
    "print(log_tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VL9taLbrI641"
   },
   "source": [
    "### Presentamos los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeRx8eSThbuq"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "heads3 = test_samples[0]\n",
    "heads4 = test_samples[1]\n",
    "\n",
    "t3 = PrettyTable(heads3)\n",
    "body3 = log_tokens1[0][:len(test_samples[0])]\n",
    "t3.add_row(body3)\n",
    "print (t3)\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "t4 = PrettyTable(heads4)\n",
    "body4 = log_tokens1[1][:len(test_samples[1])]\n",
    "t4.add_row(body2)\n",
    "print (t4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocesamiento-Copy1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
